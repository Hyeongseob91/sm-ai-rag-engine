{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 전처리 파이프라인 설계\n",
    "\n",
    "이 노트북은 RAG 시스템의 전처리 파이프라인을 설계하고 검증합니다.\n",
    "\n",
    "## 목차\n",
    "\n",
    "0. **Introduction & Setup** - 환경 설정 및 라이브러리 로드\n",
    "1. **File Parser Design** - 다양한 파일 형식 파싱 (PDF, DOCX, XLSX, TXT, JSON)\n",
    "2. **Text Normalization** - 텍스트 정규화 및 전처리\n",
    "3. **Semantic Chunking Design** - LangChain 기반 시맨틱 청킹\n",
    "4. **Metadata Management** - 메타데이터 스키마 및 관리\n",
    "5. **Weaviate Integration** - 벡터 저장소 확장 및 통합\n",
    "6. **End-to-End Pipeline** - 전체 파이프라인 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 0: Introduction & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 목적 및 개요\n",
    "\n",
    "**목적**: RAG 시스템에서 사용할 문서 전처리 파이프라인 설계 및 검증\n",
    "\n",
    "**처리 흐름**:\n",
    "```\n",
    "파일 입력 → 파서 선택 → 텍스트 추출 → 정규화 → 시맨틱 청킹 → 메타데이터 추가 → Weaviate 저장\n",
    "```\n",
    "\n",
    "**지원 파일 형식**:\n",
    "- PDF: pdfplumber\n",
    "- DOCX: python-docx\n",
    "- XLSX: openpyxl\n",
    "- TXT: 기본 파일 읽기\n",
    "- JSON: json 모듈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 필요 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 설치 확인 (이미 pyproject.toml에 추가됨)\n",
    "# !uv sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 환경 변수 로드 및 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 라이브러리 로드 완료!\n"
     ]
    }
   ],
   "source": [
    "# 표준 라이브러리\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from abc import ABC, abstractmethod\n",
    "import uuid\n",
    "\n",
    "# 파일 파싱 라이브러리\n",
    "import pdfplumber\n",
    "from docx import Document\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# LangChain\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Pydantic\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 환경 변수\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"모든 라이브러리 로드 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 디렉토리: /mnt/data1/work/sm-ai-v2/my-rag-server/dev_v1/test_data\n",
      "\n",
      "테스트 파일 목록:\n",
      "  - sample.pdf (3,957 bytes)\n",
      "  - sample.xlsx (7,661 bytes)\n",
      "  - sample.docx (37,950 bytes)\n",
      "  - sample.json (3,516 bytes)\n",
      "  - sample.txt (2,625 bytes)\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 경로 설정\n",
    "TEST_DATA_DIR = Path(\"./test_data\")\n",
    "print(f\"테스트 데이터 디렉토리: {TEST_DATA_DIR.absolute()}\")\n",
    "print(f\"\\n테스트 파일 목록:\")\n",
    "for f in TEST_DATA_DIR.iterdir():\n",
    "    print(f\"  - {f.name} ({f.stat().st_size:,} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: File Parser Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 RawDocument 데이터 클래스\n",
    "\n",
    "파서의 출력을 표준화하기 위한 데이터 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RawDocument:\n",
    "    \"\"\"파서에서 추출된 원본 문서 데이터\"\"\"\n",
    "    content: str                          # 추출된 전체 텍스트\n",
    "    source: str                           # 파일 경로\n",
    "    file_type: str                        # 파일 확장자 (pdf, docx, xlsx, txt, json)\n",
    "    file_name: str                        # 파일 이름\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)  # 파일별 메타데이터\n",
    "    pages: Optional[List[str]] = None     # 페이지별 텍스트 (PDF, DOCX)\n",
    "    sheets: Optional[Dict[str, str]] = None  # 시트별 텍스트 (XLSX)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.pages is None:\n",
    "            self.pages = []\n",
    "        if self.sheets is None:\n",
    "            self.sheets = {}\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"RawDocument(\\n\"\n",
    "            f\"  file_name='{self.file_name}',\\n\"\n",
    "            f\"  file_type='{self.file_type}',\\n\"\n",
    "            f\"  content_length={len(self.content)},\\n\"\n",
    "            f\"  pages={len(self.pages)},\\n\"\n",
    "            f\"  sheets={list(self.sheets.keys()) if self.sheets else []},\\n\"\n",
    "            f\"  metadata={self.metadata}\\n\"\n",
    "            f\")\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 BaseParser 추상 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseParser(ABC):\n",
    "    \"\"\"모든 파서의 기본 클래스\"\"\"\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def supported_extensions(self) -> List[str]:\n",
    "        \"\"\"지원하는 파일 확장자 목록\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def parse(self, file_path: str) -> RawDocument:\n",
    "        \"\"\"파일을 파싱하여 RawDocument 반환\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def can_parse(self, file_path: str) -> bool:\n",
    "        \"\"\"파일 파싱 가능 여부 확인\"\"\"\n",
    "        ext = Path(file_path).suffix.lower().lstrip('.')\n",
    "        return ext in self.supported_extensions\n",
    "    \n",
    "    def _get_file_info(self, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"파일 기본 정보 추출\"\"\"\n",
    "        path = Path(file_path)\n",
    "        stat = path.stat()\n",
    "        return {\n",
    "            \"file_name\": path.name,\n",
    "            \"file_type\": path.suffix.lower().lstrip('.'),\n",
    "            \"file_size\": stat.st_size,\n",
    "            \"created_at\": datetime.fromtimestamp(stat.st_ctime).isoformat(),\n",
    "            \"modified_at\": datetime.fromtimestamp(stat.st_mtime).isoformat(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 PDFParser (pdfplumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFParser(BaseParser):\n",
    "    \"\"\"PDF 파일 파서\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def supported_extensions(self) -> List[str]:\n",
    "        return [\"pdf\"]\n",
    "    \n",
    "    def parse(self, file_path: str) -> RawDocument:\n",
    "        file_info = self._get_file_info(file_path)\n",
    "        pages = []\n",
    "        \n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                pages.append(text)\n",
    "                \n",
    "                # 테이블 추출 (있는 경우)\n",
    "                tables = page.extract_tables()\n",
    "                for table in tables:\n",
    "                    if table:\n",
    "                        table_text = self._table_to_text(table)\n",
    "                        if table_text not in text:\n",
    "                            pages[-1] += f\"\\n\\n[Table]\\n{table_text}\"\n",
    "            \n",
    "            file_info[\"page_count\"] = len(pdf.pages)\n",
    "            file_info[\"pdf_metadata\"] = pdf.metadata or {}\n",
    "        \n",
    "        content = \"\\n\\n\".join(pages)\n",
    "        \n",
    "        return RawDocument(\n",
    "            content=content,\n",
    "            source=str(Path(file_path).absolute()),\n",
    "            file_type=\"pdf\",\n",
    "            file_name=file_info[\"file_name\"],\n",
    "            metadata=file_info,\n",
    "            pages=pages\n",
    "        )\n",
    "    \n",
    "    def _table_to_text(self, table: List[List]) -> str:\n",
    "        \"\"\"테이블을 텍스트로 변환\"\"\"\n",
    "        rows = []\n",
    "        for row in table:\n",
    "            cells = [str(cell or \"\").strip() for cell in row]\n",
    "            rows.append(\" | \".join(cells))\n",
    "        return \"\\n\".join(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 DOCXParser (python-docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DOCXParser(BaseParser):\n",
    "    \"\"\"DOCX 파일 파서\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def supported_extensions(self) -> List[str]:\n",
    "        return [\"docx\"]\n",
    "    \n",
    "    def parse(self, file_path: str) -> RawDocument:\n",
    "        file_info = self._get_file_info(file_path)\n",
    "        doc = Document(file_path)\n",
    "        \n",
    "        paragraphs = []\n",
    "        \n",
    "        # 문단 추출\n",
    "        for para in doc.paragraphs:\n",
    "            text = para.text.strip()\n",
    "            if text:\n",
    "                paragraphs.append(text)\n",
    "        \n",
    "        # 테이블 추출\n",
    "        for table in doc.tables:\n",
    "            table_text = self._extract_table(table)\n",
    "            if table_text:\n",
    "                paragraphs.append(f\"[Table]\\n{table_text}\")\n",
    "        \n",
    "        content = \"\\n\\n\".join(paragraphs)\n",
    "        \n",
    "        # 문서 속성 추출\n",
    "        props = doc.core_properties\n",
    "        file_info[\"title\"] = props.title or \"\"\n",
    "        file_info[\"author\"] = props.author or \"\"\n",
    "        file_info[\"paragraph_count\"] = len(doc.paragraphs)\n",
    "        file_info[\"table_count\"] = len(doc.tables)\n",
    "        \n",
    "        return RawDocument(\n",
    "            content=content,\n",
    "            source=str(Path(file_path).absolute()),\n",
    "            file_type=\"docx\",\n",
    "            file_name=file_info[\"file_name\"],\n",
    "            metadata=file_info,\n",
    "            pages=paragraphs  # 문단을 페이지처럼 취급\n",
    "        )\n",
    "    \n",
    "    def _extract_table(self, table) -> str:\n",
    "        \"\"\"테이블을 텍스트로 변환\"\"\"\n",
    "        rows = []\n",
    "        for row in table.rows:\n",
    "            cells = [cell.text.strip() for cell in row.cells]\n",
    "            rows.append(\" | \".join(cells))\n",
    "        return \"\\n\".join(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 XLSXParser (openpyxl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLSXParser(BaseParser):\n",
    "    \"\"\"XLSX 파일 파서\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def supported_extensions(self) -> List[str]:\n",
    "        return [\"xlsx\", \"xls\"]\n",
    "    \n",
    "    def parse(self, file_path: str) -> RawDocument:\n",
    "        file_info = self._get_file_info(file_path)\n",
    "        wb = load_workbook(file_path, read_only=True, data_only=True)\n",
    "        \n",
    "        sheets = {}\n",
    "        all_content = []\n",
    "        \n",
    "        for sheet_name in wb.sheetnames:\n",
    "            ws = wb[sheet_name]\n",
    "            sheet_content = self._extract_sheet(ws)\n",
    "            \n",
    "            if sheet_content.strip():\n",
    "                sheets[sheet_name] = sheet_content\n",
    "                all_content.append(f\"[Sheet: {sheet_name}]\\n{sheet_content}\")\n",
    "        \n",
    "        content = \"\\n\\n\".join(all_content)\n",
    "        \n",
    "        file_info[\"sheet_count\"] = len(wb.sheetnames)\n",
    "        file_info[\"sheet_names\"] = wb.sheetnames\n",
    "        \n",
    "        wb.close()\n",
    "        \n",
    "        return RawDocument(\n",
    "            content=content,\n",
    "            source=str(Path(file_path).absolute()),\n",
    "            file_type=\"xlsx\",\n",
    "            file_name=file_info[\"file_name\"],\n",
    "            metadata=file_info,\n",
    "            sheets=sheets\n",
    "        )\n",
    "    \n",
    "    def _extract_sheet(self, ws) -> str:\n",
    "        \"\"\"시트 내용을 텍스트로 변환\"\"\"\n",
    "        rows = []\n",
    "        for row in ws.iter_rows(values_only=True):\n",
    "            cells = [str(cell) if cell is not None else \"\" for cell in row]\n",
    "            if any(cells):  # 빈 행 제외\n",
    "                rows.append(\" | \".join(cells))\n",
    "        return \"\\n\".join(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 TXTParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TXTParser(BaseParser):\n",
    "    \"\"\"TXT 파일 파서\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def supported_extensions(self) -> List[str]:\n",
    "        return [\"txt\", \"md\", \"rst\"]\n",
    "    \n",
    "    def parse(self, file_path: str) -> RawDocument:\n",
    "        file_info = self._get_file_info(file_path)\n",
    "        \n",
    "        # 인코딩 자동 감지 시도\n",
    "        encodings = ['utf-8', 'cp949', 'euc-kr', 'latin-1']\n",
    "        content = None\n",
    "        \n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding) as f:\n",
    "                    content = f.read()\n",
    "                file_info[\"encoding\"] = encoding\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        \n",
    "        if content is None:\n",
    "            raise ValueError(f\"파일 인코딩을 감지할 수 없습니다: {file_path}\")\n",
    "        \n",
    "        file_info[\"line_count\"] = content.count('\\n') + 1\n",
    "        file_info[\"char_count\"] = len(content)\n",
    "        \n",
    "        return RawDocument(\n",
    "            content=content,\n",
    "            source=str(Path(file_path).absolute()),\n",
    "            file_type=file_info[\"file_type\"],\n",
    "            file_name=file_info[\"file_name\"],\n",
    "            metadata=file_info\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 JSONParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONParser(BaseParser):\n",
    "    \"\"\"JSON 파일 파서\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def supported_extensions(self) -> List[str]:\n",
    "        return [\"json\"]\n",
    "    \n",
    "    def parse(self, file_path: str) -> RawDocument:\n",
    "        file_info = self._get_file_info(file_path)\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # JSON을 읽기 쉬운 텍스트로 변환\n",
    "        content = self._json_to_text(data)\n",
    "        \n",
    "        file_info[\"json_type\"] = type(data).__name__\n",
    "        if isinstance(data, dict):\n",
    "            file_info[\"top_level_keys\"] = list(data.keys())\n",
    "        elif isinstance(data, list):\n",
    "            file_info[\"item_count\"] = len(data)\n",
    "        \n",
    "        return RawDocument(\n",
    "            content=content,\n",
    "            source=str(Path(file_path).absolute()),\n",
    "            file_type=\"json\",\n",
    "            file_name=file_info[\"file_name\"],\n",
    "            metadata=file_info\n",
    "        )\n",
    "    \n",
    "    def _json_to_text(self, data: Any, prefix: str = \"\") -> str:\n",
    "        \"\"\"JSON 데이터를 텍스트로 변환 (계층적 구조 유지)\"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                if isinstance(value, (dict, list)):\n",
    "                    lines.append(f\"{prefix}{key}:\")\n",
    "                    lines.append(self._json_to_text(value, prefix + \"  \"))\n",
    "                else:\n",
    "                    lines.append(f\"{prefix}{key}: {value}\")\n",
    "        elif isinstance(data, list):\n",
    "            for i, item in enumerate(data):\n",
    "                if isinstance(item, (dict, list)):\n",
    "                    lines.append(f\"{prefix}[{i}]:\")\n",
    "                    lines.append(self._json_to_text(item, prefix + \"  \"))\n",
    "                else:\n",
    "                    lines.append(f\"{prefix}- {item}\")\n",
    "        else:\n",
    "            lines.append(f\"{prefix}{data}\")\n",
    "        \n",
    "        return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 UnifiedFileParser (통합 인터페이스)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedFileParser:\n",
    "    \"\"\"통합 파일 파서 - 파일 확장자에 따라 적절한 파서 선택\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._parsers: List[BaseParser] = [\n",
    "            PDFParser(),\n",
    "            DOCXParser(),\n",
    "            XLSXParser(),\n",
    "            TXTParser(),\n",
    "            JSONParser(),\n",
    "        ]\n",
    "    \n",
    "    def get_supported_extensions(self) -> List[str]:\n",
    "        \"\"\"지원하는 모든 파일 확장자 반환\"\"\"\n",
    "        extensions = []\n",
    "        for parser in self._parsers:\n",
    "            extensions.extend(parser.supported_extensions)\n",
    "        return extensions\n",
    "    \n",
    "    def parse(self, file_path: str) -> RawDocument:\n",
    "        \"\"\"파일을 파싱하여 RawDocument 반환\"\"\"\n",
    "        for parser in self._parsers:\n",
    "            if parser.can_parse(file_path):\n",
    "                return parser.parse(file_path)\n",
    "        \n",
    "        ext = Path(file_path).suffix\n",
    "        raise ValueError(f\"지원하지 않는 파일 형식입니다: {ext}\")\n",
    "    \n",
    "    def parse_directory(self, dir_path: str, recursive: bool = False) -> List[RawDocument]:\n",
    "        \"\"\"디렉토리 내 모든 지원 파일 파싱\"\"\"\n",
    "        documents = []\n",
    "        path = Path(dir_path)\n",
    "        \n",
    "        pattern = \"**/*\" if recursive else \"*\"\n",
    "        \n",
    "        for file_path in path.glob(pattern):\n",
    "            if file_path.is_file():\n",
    "                ext = file_path.suffix.lower().lstrip('.')\n",
    "                if ext in self.get_supported_extensions():\n",
    "                    try:\n",
    "                        doc = self.parse(str(file_path))\n",
    "                        documents.append(doc)\n",
    "                        print(f\"  ✓ 파싱 완료: {file_path.name}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ✗ 파싱 실패: {file_path.name} - {e}\")\n",
    "        \n",
    "        return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 파서 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지원 확장자: ['pdf', 'docx', 'xlsx', 'xls', 'txt', 'md', 'rst', 'json']\n"
     ]
    }
   ],
   "source": [
    "# 통합 파서 생성\n",
    "parser = UnifiedFileParser()\n",
    "print(f\"지원 확장자: {parser.get_supported_extensions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "테스트 파일 파싱 결과\n",
      "==================================================\n",
      "  ✓ 파싱 완료: sample.pdf\n",
      "  ✓ 파싱 완료: sample.xlsx\n",
      "  ✓ 파싱 완료: sample.docx\n",
      "  ✓ 파싱 완료: sample.json\n",
      "  ✓ 파싱 완료: sample.txt\n",
      "\n",
      "총 5개 파일 파싱 완료\n"
     ]
    }
   ],
   "source": [
    "# 테스트 파일 파싱\n",
    "print(\"=\" * 50)\n",
    "print(\"테스트 파일 파싱 결과\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "documents = parser.parse_directory(str(TEST_DATA_DIR))\n",
    "\n",
    "print(f\"\\n총 {len(documents)}개 파일 파싱 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RawDocument(\n",
      "  file_name='sample.pdf',\n",
      "  file_type='pdf',\n",
      "  content_length=1957,\n",
      "  pages=2,\n",
      "  sheets=[],\n",
      "  metadata={'file_name': 'sample.pdf', 'file_type': 'pdf', 'file_size': 3957, 'created_at': '2025-12-11T10:09:32.435347', 'modified_at': '2025-12-09T16:20:56.788867', 'page_count': 2, 'pdf_metadata': {'Author': 'anonymous', 'CreationDate': \"D:20251209162056+09'00'\", 'Creator': 'ReportLab PDF Library - www.reportlab.com', 'Keywords': '', 'ModDate': \"D:20251209162056+09'00'\", 'Producer': 'ReportLab PDF Library - www.reportlab.com', 'Subject': 'unspecified', 'Title': 'untitled', 'Trapped': 'False'}}\n",
      ")\n",
      "\n",
      "--- 내용 미리보기 (처음 500자) ---\n",
      "RAG System Technical Report\n",
      "1. Executive Summary\n",
      "This document describes the RAG (Retrieval-Augmented Generation) system\n",
      "architecture and implementation details. The system is designed to enhance\n",
      "LLM responses by retrieving relevant information from a vector database.\n",
      "Key Features:\n",
      "- Multi-format document parsing (PDF, DOCX, XLSX, TXT, JSON)\n",
      "- Semantic chunking for optimal context retrieval\n",
      "- Hybrid search with BM25 and dense vectors\n",
      "- Cross-encoder reranking for improved accuracy\n",
      "2. System Arch...\n",
      "\n",
      "==================================================\n",
      "RawDocument(\n",
      "  file_name='sample.xlsx',\n",
      "  file_type='xlsx',\n",
      "  content_length=1178,\n",
      "  pages=0,\n",
      "  sheets=['프로젝트 일정', '테스트 결과', '성능 메트릭'],\n",
      "  metadata={'file_name': 'sample.xlsx', 'file_type': 'xlsx', 'file_size': 7661, 'created_at': '2025-12-11T10:09:32.488348', 'modified_at': '2025-12-09T16:20:56.765866', 'sheet_count': 3, 'sheet_names': ['프로젝트 일정', '테스트 결과', '성능 메트릭']}\n",
      ")\n",
      "\n",
      "--- 내용 미리보기 (처음 500자) ---\n",
      "[Sheet: 프로젝트 일정]\n",
      "단계 | 작업 내용 | 담당자 | 시작일 | 종료일 | 상태\n",
      "1단계 | 요구사항 분석 | 김철수 | 2024-01-15 | 2024-01-31 | 완료\n",
      "1단계 | 시스템 설계 | 이영희 | 2024-02-01 | 2024-02-15 | 완료\n",
      "2단계 | 문서 파서 개발 | 박민수 | 2024-02-16 | 2024-03-15 | 완료\n",
      "2단계 | 청킹 서비스 개발 | 최지은 | 2024-03-01 | 2024-03-20 | 진행중\n",
      "2단계 | Weaviate 연동 | 김철수 | 2024-03-10 | 2024-03-25 | 진행중\n",
      "3단계 | 테스트 및 검증 | 전체팀 | 2024-03-26 | 2024-04-10 | 예정\n",
      "3단계 | 성능 최적화 | 박민수 | 2024-04-11 | 2024-04-25 | 예정\n",
      "4단계 | 배포 준비 | 이영희 | 2024-04-26 | 2024-05-10 | 예정\n",
      "\n",
      "[Sheet: 테스트 결과]\n",
      "테스트 ID | 테스트 항목 | ...\n",
      "\n",
      "==================================================\n",
      "RawDocument(\n",
      "  file_name='sample.docx',\n",
      "  file_type='docx',\n",
      "  content_length=878,\n",
      "  pages=19,\n",
      "  sheets=[],\n",
      "  metadata={'file_name': 'sample.docx', 'file_type': 'docx', 'file_size': 37950, 'created_at': '2025-12-11T10:09:32.297345', 'modified_at': '2025-12-09T16:20:56.755866', 'title': '', 'author': 'python-docx', 'paragraph_count': 18, 'table_count': 1}\n",
      ")\n",
      "\n",
      "--- 내용 미리보기 (처음 500자) ---\n",
      "AI 프로젝트 기술 명세서\n",
      "\n",
      "1. 프로젝트 개요\n",
      "\n",
      "RAG(Retrieval-Augmented Generation) 시스템은 대규모 언어 모델(LLM)의 한계를 극복하기 위한 아키텍처입니다. 기존 LLM이 학습 데이터에만 의존하는 것과 달리, RAG는 외부 지식 베이스에서 관련 정보를 검색하여 답변 생성에 활용합니다.\n",
      "\n",
      "이 시스템의 주요 장점은 다음과 같습니다:\n",
      "\n",
      "최신 정보 반영 가능\n",
      "\n",
      "환각(Hallucination) 현상 감소\n",
      "\n",
      "출처 명시로 신뢰성 향상\n",
      "\n",
      "도메인 특화 지식 활용\n",
      "\n",
      "2. 시스템 아키텍처\n",
      "\n",
      "RAG 시스템은 크게 세 가지 컴포넌트로 구성됩니다: 문서 전처리 파이프라인, 벡터 데이터베이스, 그리고 생성 모델입니다.\n",
      "\n",
      "2.1 문서 전처리\n",
      "\n",
      "문서 전처리 단계에서는 다양한 형식의 문서(PDF, DOCX, XLSX 등)를 파싱하고, 시맨틱 청킹을 통해 의미 단위로 분할합니다. 이 과정에서 메타데이터를 추출하여 검색 효율성을 높입니다.\n",
      "\n",
      "2.2 벡터 데이터베이스\n",
      "\n",
      "Weaviate를 ...\n",
      "\n",
      "==================================================\n",
      "RawDocument(\n",
      "  file_name='sample.json',\n",
      "  file_type='json',\n",
      "  content_length=2111,\n",
      "  pages=0,\n",
      "  sheets=[],\n",
      "  metadata={'file_name': 'sample.json', 'file_type': 'json', 'file_size': 3516, 'created_at': '2025-12-11T10:09:32.363346', 'modified_at': '2025-12-09T16:19:12.777327', 'json_type': 'dict', 'top_level_keys': ['company', 'products', 'support', 'partners']}\n",
      ")\n",
      "\n",
      "--- 내용 미리보기 (처음 500자) ---\n",
      "company:\n",
      "  name: 스마트AI 솔루션즈\n",
      "  founded: 2020\n",
      "  headquarters: 서울 강남구\n",
      "products:\n",
      "  [0]:\n",
      "    id: PROD-001\n",
      "    name: AI 문서 분석기\n",
      "    category: NLP\n",
      "    description: PDF, Word, Excel 등 다양한 문서 형식을 자동으로 분석하고 핵심 정보를 추출하는 AI 솔루션입니다. 계약서, 보고서, 이력서 등 다양한 문서 유형을 지원합니다.\n",
      "    features:\n",
      "      - OCR 기반 텍스트 추출\n",
      "      - 개체명 인식 (NER)\n",
      "      - 문서 요약 생성\n",
      "      - 키워드 추출\n",
      "      - 다국어 지원 (한국어, 영어, 일본어, 중국어)\n",
      "    pricing:\n",
      "      basic:\n",
      "        price: 500000\n",
      "        currency: KRW\n",
      "        period: monthly\n",
      "        documents_limit: 1000...\n",
      "\n",
      "==================================================\n",
      "RawDocument(\n",
      "  file_name='sample.txt',\n",
      "  file_type='txt',\n",
      "  content_length=1307,\n",
      "  pages=0,\n",
      "  sheets=[],\n",
      "  metadata={'file_name': 'sample.txt', 'file_type': 'txt', 'file_size': 2625, 'created_at': '2025-12-11T10:09:32.472347', 'modified_at': '2025-12-09T16:19:12.667325', 'encoding': 'utf-8', 'line_count': 65, 'char_count': 1307}\n",
      ")\n",
      "\n",
      "--- 내용 미리보기 (처음 500자) ---\n",
      "# 스마트AI 솔루션즈 회사 정책 문서\n",
      "\n",
      "## 1. 회사 개요\n",
      "\n",
      "스마트AI 솔루션즈는 2020년에 설립된 인공지능 전문 기업입니다. 우리는 기업용 AI 솔루션을 개발하고 제공하며, 특히 자연어 처리(NLP)와 컴퓨터 비전 분야에서 선도적인 기술력을 보유하고 있습니다.\n",
      "\n",
      "본사는 서울 강남구에 위치하고 있으며, 현재 150명의 직원이 근무하고 있습니다. 연구개발팀, 프로덕트팀, 영업팀, 경영지원팀으로 구성되어 있습니다.\n",
      "\n",
      "## 2. 근무 정책\n",
      "\n",
      "### 2.1 근무 시간\n",
      "- 기본 근무 시간: 오전 9시 ~ 오후 6시 (점심시간 12시~1시)\n",
      "- 유연 근무제 적용: 코어 타임 10시~4시 내 필수 근무\n",
      "- 재택 근무: 주 2회까지 가능 (팀장 승인 필요)\n",
      "\n",
      "### 2.2 휴가 정책\n",
      "- 연차 휴가: 입사 1년차 15일, 이후 매년 1일씩 추가 (최대 25일)\n",
      "- 경조사 휴가: 결혼 5일, 출산 10일 (배우자 3일), 사망 3~5일\n",
      "- 리프레시 휴가: 3년 근속 시 5일 추가 지급\n",
      "\n",
      "### ...\n"
     ]
    }
   ],
   "source": [
    "# 각 문서 상세 정보 출력\n",
    "for doc in documents:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(doc)\n",
    "    print(\"\\n--- 내용 미리보기 (처음 500자) ---\")\n",
    "    print(doc.content[:500] + \"...\" if len(doc.content) > 500 else doc.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 TextNormalizer 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer:\n",
    "    \"\"\"텍스트 정규화 클래스\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        remove_extra_whitespace: bool = True,\n",
    "        remove_extra_newlines: bool = True,\n",
    "        remove_special_chars: bool = False,\n",
    "        lowercase: bool = False,\n",
    "        min_line_length: int = 0\n",
    "    ):\n",
    "        self.remove_extra_whitespace = remove_extra_whitespace\n",
    "        self.remove_extra_newlines = remove_extra_newlines\n",
    "        self.remove_special_chars = remove_special_chars\n",
    "        self.lowercase = lowercase\n",
    "        self.min_line_length = min_line_length\n",
    "    \n",
    "    def normalize(self, text: str) -> str:\n",
    "        \"\"\"텍스트 정규화 수행\"\"\"\n",
    "        result = text\n",
    "        \n",
    "        # 1. 여러 줄바꿈을 2개로 통일\n",
    "        if self.remove_extra_newlines:\n",
    "            result = re.sub(r'\\n{3,}', '\\n\\n', result)\n",
    "        \n",
    "        # 2. 여러 공백을 하나로\n",
    "        if self.remove_extra_whitespace:\n",
    "            result = re.sub(r'[ \\t]+', ' ', result)\n",
    "            result = re.sub(r' +\\n', '\\n', result)  # 줄 끝 공백 제거\n",
    "        \n",
    "        # 3. 특수 문자 제거 (선택적)\n",
    "        if self.remove_special_chars:\n",
    "            # 한글, 영문, 숫자, 기본 구두점만 유지\n",
    "            result = re.sub(r'[^가-힣a-zA-Z0-9\\s.,!?\\-:;()\\[\\]\"\\']', '', result)\n",
    "        \n",
    "        # 4. 소문자 변환 (선택적)\n",
    "        if self.lowercase:\n",
    "            result = result.lower()\n",
    "        \n",
    "        # 5. 짧은 줄 필터링 (선택적)\n",
    "        if self.min_line_length > 0:\n",
    "            lines = result.split('\\n')\n",
    "            lines = [line for line in lines if len(line.strip()) >= self.min_line_length or line.strip() == '']\n",
    "            result = '\\n'.join(lines)\n",
    "        \n",
    "        return result.strip()\n",
    "    \n",
    "    def normalize_document(self, doc: RawDocument) -> RawDocument:\n",
    "        \"\"\"RawDocument의 내용을 정규화\"\"\"\n",
    "        normalized_content = self.normalize(doc.content)\n",
    "        normalized_pages = [self.normalize(p) for p in doc.pages] if doc.pages else None\n",
    "        normalized_sheets = {k: self.normalize(v) for k, v in doc.sheets.items()} if doc.sheets else None\n",
    "        \n",
    "        return RawDocument(\n",
    "            content=normalized_content,\n",
    "            source=doc.source,\n",
    "            file_type=doc.file_type,\n",
    "            file_name=doc.file_name,\n",
    "            metadata={**doc.metadata, \"normalized\": True},\n",
    "            pages=normalized_pages,\n",
    "            sheets=normalized_sheets\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 정규화 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 원본 ===\n",
      "'\\n이것은    테스트   문장입니다.\\n\\n\\n\\n여러 줄바꿈이     있습니다.\\n\\na\\nb\\n\\n긴 문장은 유지됩니다.\\n'\n",
      "\n",
      "=== 정규화 후 ===\n",
      "'이것은 테스트 문장입니다.\\n\\n여러 줄바꿈이 있습니다.\\n\\n\\n긴 문장은 유지됩니다.'\n"
     ]
    }
   ],
   "source": [
    "# 정규화기 생성\n",
    "normalizer = TextNormalizer(\n",
    "    remove_extra_whitespace=True,\n",
    "    remove_extra_newlines=True,\n",
    "    min_line_length=3\n",
    ")\n",
    "\n",
    "# 테스트 텍스트\n",
    "test_text = \"\"\"\n",
    "이것은    테스트   문장입니다.\n",
    "\n",
    "\n",
    "\n",
    "여러 줄바꿈이     있습니다.\n",
    "\n",
    "a\n",
    "b\n",
    "\n",
    "긴 문장은 유지됩니다.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== 원본 ===\")\n",
    "print(repr(test_text))\n",
    "print(\"\\n=== 정규화 후 ===\")\n",
    "print(repr(normalizer.normalize(test_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample.pdf: 1,957 → 1,957 (0.0% 감소)\n",
      "sample.xlsx: 1,178 → 1,178 (0.0% 감소)\n",
      "sample.docx: 878 → 878 (0.0% 감소)\n",
      "sample.json: 2,111 → 1,731 (18.0% 감소)\n",
      "sample.txt: 1,307 → 1,306 (0.1% 감소)\n"
     ]
    }
   ],
   "source": [
    "# 문서 정규화\n",
    "normalized_docs = [normalizer.normalize_document(doc) for doc in documents]\n",
    "\n",
    "# 비교\n",
    "for orig, norm in zip(documents, normalized_docs):\n",
    "    orig_len = len(orig.content)\n",
    "    norm_len = len(norm.content)\n",
    "    reduction = (1 - norm_len / orig_len) * 100 if orig_len > 0 else 0\n",
    "    print(f\"{orig.file_name}: {orig_len:,} → {norm_len:,} ({reduction:.1f}% 감소)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Semantic Chunking Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 LangChain SemanticChunker 소개\n",
    "\n",
    "SemanticChunker는 임베딩 유사도를 기반으로 텍스트를 의미 단위로 분할합니다.\n",
    "\n",
    "**Breakpoint 전략:**\n",
    "- `percentile`: 상위 N%의 유사도 차이에서 분할 (기본값)\n",
    "- `standard_deviation`: 평균 + N*표준편차 이상에서 분할\n",
    "- `interquartile`: IQR 기반 분할\n",
    "- `gradient`: 유사도 변화율 기반 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Chunk 데이터 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"청킹된 텍스트 단위\"\"\"\n",
    "    content: str                    # 청크 텍스트\n",
    "    chunk_id: str                   # 고유 ID\n",
    "    chunk_index: int                # 문서 내 순서\n",
    "    doc_id: str                     # 원본 문서 ID\n",
    "    source: str                     # 원본 파일 경로\n",
    "    file_name: str                  # 파일 이름\n",
    "    file_type: str                  # 파일 형식\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    @property\n",
    "    def char_count(self) -> int:\n",
    "        return len(self.content)\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"content\": self.content,\n",
    "            \"chunk_id\": self.chunk_id,\n",
    "            \"chunk_index\": self.chunk_index,\n",
    "            \"doc_id\": self.doc_id,\n",
    "            \"source\": self.source,\n",
    "            \"file_name\": self.file_name,\n",
    "            \"file_type\": self.file_type,\n",
    "            \"char_count\": self.char_count,\n",
    "            **self.metadata\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ChunkingService 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkingService:\n",
    "    \"\"\"시맨틱 청킹 서비스\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: str = \"text-embedding-3-small\",\n",
    "        breakpoint_type: str = \"percentile\",\n",
    "        breakpoint_threshold: float = 95,\n",
    "        min_chunk_size: int = 100,\n",
    "        max_chunk_size: int = 2000\n",
    "    ):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.breakpoint_type = breakpoint_type\n",
    "        self.breakpoint_threshold = breakpoint_threshold\n",
    "        self.min_chunk_size = min_chunk_size\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        \n",
    "        # 임베딩 모델 초기화\n",
    "        self._embeddings = OpenAIEmbeddings(model=embedding_model)\n",
    "        \n",
    "        # SemanticChunker 초기화\n",
    "        self._chunker = SemanticChunker(\n",
    "            embeddings=self._embeddings,\n",
    "            breakpoint_threshold_type=breakpoint_type,\n",
    "            breakpoint_threshold_amount=breakpoint_threshold\n",
    "        )\n",
    "    \n",
    "    def chunk_text(self, text: str, doc_id: str, source: str, file_name: str, file_type: str) -> List[Chunk]:\n",
    "        \"\"\"텍스트를 시맨틱 청킹\"\"\"\n",
    "        # SemanticChunker로 분할\n",
    "        langchain_docs = self._chunker.create_documents([text])\n",
    "        \n",
    "        chunks = []\n",
    "        for i, lc_doc in enumerate(langchain_docs):\n",
    "            content = lc_doc.page_content\n",
    "            \n",
    "            # 최소 크기 미달 청크 병합은 여기서 처리 가능\n",
    "            # (현재는 그대로 유지)\n",
    "            \n",
    "            chunk = Chunk(\n",
    "                content=content,\n",
    "                chunk_id=str(uuid.uuid4()),\n",
    "                chunk_index=i,\n",
    "                doc_id=doc_id,\n",
    "                source=source,\n",
    "                file_name=file_name,\n",
    "                file_type=file_type,\n",
    "                metadata={\n",
    "                    \"chunking_method\": \"semantic\",\n",
    "                    \"breakpoint_type\": self.breakpoint_type,\n",
    "                    \"breakpoint_threshold\": self.breakpoint_threshold\n",
    "                }\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def chunk_document(self, doc: RawDocument) -> List[Chunk]:\n",
    "        \"\"\"RawDocument를 시맨틱 청킹\"\"\"\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        return self.chunk_text(\n",
    "            text=doc.content,\n",
    "            doc_id=doc_id,\n",
    "            source=doc.source,\n",
    "            file_name=doc.file_name,\n",
    "            file_type=doc.file_type\n",
    "        )\n",
    "    \n",
    "    def get_chunk_stats(self, chunks: List[Chunk]) -> Dict[str, Any]:\n",
    "        \"\"\"청킹 통계 반환\"\"\"\n",
    "        if not chunks:\n",
    "            return {\"count\": 0}\n",
    "        \n",
    "        sizes = [c.char_count for c in chunks]\n",
    "        return {\n",
    "            \"count\": len(chunks),\n",
    "            \"total_chars\": sum(sizes),\n",
    "            \"avg_size\": sum(sizes) / len(sizes),\n",
    "            \"min_size\": min(sizes),\n",
    "            \"max_size\": max(sizes),\n",
    "            \"sizes\": sizes\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Breakpoint 전략 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 문서: sample.pdf (1,957 글자)\n",
      "\n",
      "=== Percentile (95) ===\n",
      "청크 수: 2\n",
      "평균 크기: 978자\n",
      "최소/최대: 862 / 1093자\n"
     ]
    }
   ],
   "source": [
    "# 테스트용 긴 텍스트 선택\n",
    "test_doc = max(normalized_docs, key=lambda d: len(d.content))\n",
    "print(f\"테스트 문서: {test_doc.file_name} ({len(test_doc.content):,} 글자)\")\n",
    "\n",
    "# Percentile 전략 테스트\n",
    "chunker_percentile = ChunkingService(\n",
    "    breakpoint_type=\"percentile\",\n",
    "    breakpoint_threshold=95\n",
    ")\n",
    "\n",
    "chunks_percentile = chunker_percentile.chunk_document(test_doc)\n",
    "stats_percentile = chunker_percentile.get_chunk_stats(chunks_percentile)\n",
    "\n",
    "print(f\"\\n=== Percentile (95) ===\")\n",
    "print(f\"청크 수: {stats_percentile['count']}\")\n",
    "print(f\"평균 크기: {stats_percentile['avg_size']:.0f}자\")\n",
    "print(f\"최소/최대: {stats_percentile['min_size']} / {stats_percentile['max_size']}자\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Threshold 비교 ===\n",
      "Threshold    청크 수       평균 크기        최소       최대      \n",
      "--------------------------------------------------\n",
      "90           3          651          218      1093    \n",
      "92           3          651          218      1093    \n",
      "95           2          978          862      1093    \n",
      "97           2          978          862      1093    \n",
      "99           2          978          862      1093    \n"
     ]
    }
   ],
   "source": [
    "# 다른 threshold 값 비교\n",
    "print(\"\\n=== Threshold 비교 ===\")\n",
    "print(f\"{'Threshold':<12} {'청크 수':<10} {'평균 크기':<12} {'최소':<8} {'최대':<8}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for threshold in [90, 92, 95, 97, 99]:\n",
    "    chunker = ChunkingService(breakpoint_type=\"percentile\", breakpoint_threshold=threshold)\n",
    "    chunks = chunker.chunk_document(test_doc)\n",
    "    stats = chunker.get_chunk_stats(chunks)\n",
    "    print(f\"{threshold:<12} {stats['count']:<10} {stats['avg_size']:<12.0f} {stats['min_size']:<8} {stats['max_size']:<8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 청킹 품질 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 청크 내용 미리보기 ===\n",
      "\n",
      "--- Chunk 0 (1093자) ---\n",
      "RAG System Technical Report\n",
      "1. Executive Summary\n",
      "This document describes the RAG (Retrieval-Augmented Generation) system\n",
      "architecture and implementation details. The system is designed to enhance\n",
      "LLM ...\n",
      "\n",
      "--- Chunk 1 (862자) ---\n",
      "3. Performance Metrics\n",
      "The system has been evaluated on the following metrics:\n",
      "+-------------------+----------+--------+\n",
      "| Metric | Value | Target |\n",
      "+-------------------+----------+--------+\n",
      "| Precisi...\n"
     ]
    }
   ],
   "source": [
    "# 청크 내용 미리보기\n",
    "print(\"=== 청크 내용 미리보기 ===\")\n",
    "for i, chunk in enumerate(chunks_percentile[:5]):\n",
    "    print(f\"\\n--- Chunk {i} ({chunk.char_count}자) ---\")\n",
    "    preview = chunk.content[:200] + \"...\" if len(chunk.content) > 200 else chunk.content\n",
    "    print(preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 전체 문서 청킹 ===\n",
      "sample.pdf: 2개 청크, 평균 978자\n",
      "sample.xlsx: 1개 청크, 평균 1178자\n",
      "sample.docx: 2개 청크, 평균 436자\n",
      "sample.json: 2개 청크, 평균 864자\n",
      "sample.txt: 2개 청크, 평균 650자\n",
      "\n",
      "총 9개 청크 생성\n"
     ]
    }
   ],
   "source": [
    "# 모든 문서 청킹\n",
    "print(\"\\n=== 전체 문서 청킹 ===\")\n",
    "all_chunks = []\n",
    "\n",
    "for doc in normalized_docs:\n",
    "    chunks = chunker_percentile.chunk_document(doc)\n",
    "    all_chunks.extend(chunks)\n",
    "    stats = chunker_percentile.get_chunk_stats(chunks)\n",
    "    print(f\"{doc.file_name}: {stats['count']}개 청크, 평균 {stats['avg_size']:.0f}자\")\n",
    "\n",
    "print(f\"\\n총 {len(all_chunks)}개 청크 생성\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Metadata Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Pydantic 메타데이터 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_295674/155222958.py:1: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  class DocumentMetadata(BaseModel):\n",
      "/tmp/ipykernel_295674/155222958.py:18: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  class ChunkMetadata(BaseModel):\n"
     ]
    }
   ],
   "source": [
    "class DocumentMetadata(BaseModel):\n",
    "    \"\"\"문서 메타데이터 스키마\"\"\"\n",
    "    doc_id: str = Field(description=\"문서 고유 ID\")\n",
    "    source: str = Field(description=\"파일 경로\")\n",
    "    file_name: str = Field(description=\"파일 이름\")\n",
    "    file_type: str = Field(description=\"파일 형식\")\n",
    "    file_size: int = Field(default=0, description=\"파일 크기 (bytes)\")\n",
    "    created_at: datetime = Field(default_factory=datetime.now, description=\"생성 일시\")\n",
    "    page_count: Optional[int] = Field(default=None, description=\"페이지 수 (PDF, DOCX)\")\n",
    "    sheet_count: Optional[int] = Field(default=None, description=\"시트 수 (XLSX)\")\n",
    "    \n",
    "    class Config:\n",
    "        json_encoders = {\n",
    "            datetime: lambda v: v.isoformat()\n",
    "        }\n",
    "\n",
    "\n",
    "class ChunkMetadata(BaseModel):\n",
    "    \"\"\"청크 메타데이터 스키마\"\"\"\n",
    "    chunk_id: str = Field(description=\"청크 고유 ID\")\n",
    "    doc_id: str = Field(description=\"원본 문서 ID\")\n",
    "    chunk_index: int = Field(description=\"문서 내 순서\")\n",
    "    total_chunks: int = Field(description=\"문서의 전체 청크 수\")\n",
    "    source: str = Field(description=\"원본 파일 경로\")\n",
    "    file_name: str = Field(description=\"파일 이름\")\n",
    "    file_type: str = Field(description=\"파일 형식\")\n",
    "    char_count: int = Field(description=\"청크 글자 수\")\n",
    "    page_number: Optional[int] = Field(default=None, description=\"페이지 번호 (PDF)\")\n",
    "    sheet_name: Optional[str] = Field(default=None, description=\"시트 이름 (XLSX)\")\n",
    "    created_at: datetime = Field(default_factory=datetime.now, description=\"생성 일시\")\n",
    "    \n",
    "    class Config:\n",
    "        json_encoders = {\n",
    "            datetime: lambda v: v.isoformat()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ChunkMetadata 예시 ===\n",
      "{\n",
      "  \"chunk_id\": \"847ce983-2e63-4e81-8898-789f3753b122\",\n",
      "  \"doc_id\": \"29e35b99-7915-4450-a642-2b72b571c378\",\n",
      "  \"chunk_index\": 0,\n",
      "  \"total_chunks\": 2,\n",
      "  \"source\": \"/mnt/data1/work/sm-ai-v2/my-rag-server/dev_v1/test_data/sample.pdf\",\n",
      "  \"file_name\": \"sample.pdf\",\n",
      "  \"file_type\": \"pdf\",\n",
      "  \"char_count\": 1093,\n",
      "  \"page_number\": null,\n",
      "  \"sheet_name\": null,\n",
      "  \"created_at\": \"2025-12-11T16:26:20.000206\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 메타데이터 생성 테스트\n",
    "sample_chunk = all_chunks[0]\n",
    "\n",
    "chunk_meta = ChunkMetadata(\n",
    "    chunk_id=sample_chunk.chunk_id,\n",
    "    doc_id=sample_chunk.doc_id,\n",
    "    chunk_index=sample_chunk.chunk_index,\n",
    "    total_chunks=len([c for c in all_chunks if c.doc_id == sample_chunk.doc_id]),\n",
    "    source=sample_chunk.source,\n",
    "    file_name=sample_chunk.file_name,\n",
    "    file_type=sample_chunk.file_type,\n",
    "    char_count=sample_chunk.char_count\n",
    ")\n",
    "\n",
    "print(\"=== ChunkMetadata 예시 ===\")\n",
    "print(chunk_meta.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Weaviate Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 확장된 컬렉션 스키마\n",
    "\n",
    "기존 스키마에 메타데이터 속성을 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "확장된 Weaviate 스키마:\n",
      "  - content: text\n",
      "  - chunk_id: text\n",
      "  - doc_id: text\n",
      "  - chunk_index: int\n",
      "  - total_chunks: int\n",
      "  - source: text\n",
      "  - file_name: text\n",
      "  - file_type: text\n",
      "  - char_count: int\n",
      "  - page_number: int\n",
      "  - sheet_name: text\n",
      "  - created_at: date\n"
     ]
    }
   ],
   "source": [
    "# Weaviate 설정 (참고용 - 실제 연결은 Section 6에서)\n",
    "WEAVIATE_SCHEMA = {\n",
    "    \"class\": \"DocumentChunk\",\n",
    "    \"description\": \"RAG 시스템의 문서 청크\",\n",
    "    \"properties\": [\n",
    "        # 기존 속성\n",
    "        {\"name\": \"content\", \"dataType\": [\"text\"], \"description\": \"청크 텍스트\"},\n",
    "        \n",
    "        # 신규 메타데이터 속성\n",
    "        {\"name\": \"chunk_id\", \"dataType\": [\"text\"], \"description\": \"청크 고유 ID\"},\n",
    "        {\"name\": \"doc_id\", \"dataType\": [\"text\"], \"description\": \"문서 고유 ID\"},\n",
    "        {\"name\": \"chunk_index\", \"dataType\": [\"int\"], \"description\": \"문서 내 순서\"},\n",
    "        {\"name\": \"total_chunks\", \"dataType\": [\"int\"], \"description\": \"문서의 전체 청크 수\"},\n",
    "        {\"name\": \"source\", \"dataType\": [\"text\"], \"description\": \"파일 경로\"},\n",
    "        {\"name\": \"file_name\", \"dataType\": [\"text\"], \"description\": \"파일 이름\"},\n",
    "        {\"name\": \"file_type\", \"dataType\": [\"text\"], \"description\": \"파일 형식\"},\n",
    "        {\"name\": \"char_count\", \"dataType\": [\"int\"], \"description\": \"글자 수\"},\n",
    "        {\"name\": \"page_number\", \"dataType\": [\"int\"], \"description\": \"페이지 번호\"},\n",
    "        {\"name\": \"sheet_name\", \"dataType\": [\"text\"], \"description\": \"시트 이름\"},\n",
    "        {\"name\": \"created_at\", \"dataType\": [\"date\"], \"description\": \"생성 일시\"},\n",
    "    ],\n",
    "    \"vectorizer\": \"none\",  # 외부 임베딩 사용\n",
    "}\n",
    "\n",
    "print(\"확장된 Weaviate 스키마:\")\n",
    "for prop in WEAVIATE_SCHEMA[\"properties\"]:\n",
    "    print(f\"  - {prop['name']}: {prop['dataType'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Weaviate 연결 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weaviate 연동 테스트 비활성화 (WEAVIATE_ENABLED = False)\n"
     ]
    }
   ],
   "source": [
    "# Weaviate 연결 (옵션)\n",
    "# 실제 Weaviate 서버가 실행 중인 경우에만 실행\n",
    "\n",
    "WEAVIATE_ENABLED = False  # True로 변경하여 Weaviate 연동 테스트\n",
    "\n",
    "if WEAVIATE_ENABLED:\n",
    "    import weaviate\n",
    "    from weaviate.classes.config import Property, DataType\n",
    "    \n",
    "    client = weaviate.connect_to_local(\n",
    "        host=os.getenv(\"WEAVIATE_HOST\", \"localhost\"),\n",
    "        port=int(os.getenv(\"WEAVIATE_PORT\", 8080))\n",
    "    )\n",
    "    \n",
    "    print(f\"Weaviate 연결 상태: {client.is_ready()}\")\n",
    "else:\n",
    "    print(\"Weaviate 연동 테스트 비활성화 (WEAVIATE_ENABLED = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: End-to-End Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 PreprocessingPipeline 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PreprocessingResult:\n",
    "    \"\"\"전처리 결과\"\"\"\n",
    "    document: RawDocument           # 원본 문서\n",
    "    chunks: List[Chunk]             # 청킹 결과\n",
    "    metadata: DocumentMetadata      # 문서 메타데이터\n",
    "    stats: Dict[str, Any]           # 통계\n",
    "    success: bool = True            # 처리 성공 여부\n",
    "    error: Optional[str] = None     # 에러 메시지\n",
    "\n",
    "\n",
    "class PreprocessingPipeline:\n",
    "    \"\"\"전처리 파이프라인 - 파일 → 청크 → 메타데이터 전체 처리\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        chunking_service: ChunkingService,\n",
    "        normalizer: TextNormalizer,\n",
    "        file_parser: UnifiedFileParser\n",
    "    ):\n",
    "        self.chunking_service = chunking_service\n",
    "        self.normalizer = normalizer\n",
    "        self.file_parser = file_parser\n",
    "    \n",
    "    def process_file(self, file_path: str) -> PreprocessingResult:\n",
    "        \"\"\"단일 파일 전처리\"\"\"\n",
    "        try:\n",
    "            # 1. 파싱\n",
    "            raw_doc = self.file_parser.parse(file_path)\n",
    "            \n",
    "            # 2. 정규화\n",
    "            normalized_doc = self.normalizer.normalize_document(raw_doc)\n",
    "            \n",
    "            # 3. 청킹\n",
    "            chunks = self.chunking_service.chunk_document(normalized_doc)\n",
    "            \n",
    "            # 4. 청크 메타데이터 보강\n",
    "            total_chunks = len(chunks)\n",
    "            for chunk in chunks:\n",
    "                chunk.metadata[\"total_chunks\"] = total_chunks\n",
    "            \n",
    "            # 5. 문서 메타데이터 생성\n",
    "            doc_metadata = DocumentMetadata(\n",
    "                doc_id=chunks[0].doc_id if chunks else str(uuid.uuid4()),\n",
    "                source=normalized_doc.source,\n",
    "                file_name=normalized_doc.file_name,\n",
    "                file_type=normalized_doc.file_type,\n",
    "                file_size=normalized_doc.metadata.get(\"file_size\", 0),\n",
    "                page_count=normalized_doc.metadata.get(\"page_count\"),\n",
    "                sheet_count=normalized_doc.metadata.get(\"sheet_count\")\n",
    "            )\n",
    "            \n",
    "            # 6. 통계\n",
    "            stats = self.chunking_service.get_chunk_stats(chunks)\n",
    "            stats[\"original_length\"] = len(raw_doc.content)\n",
    "            stats[\"normalized_length\"] = len(normalized_doc.content)\n",
    "            \n",
    "            return PreprocessingResult(\n",
    "                document=normalized_doc,\n",
    "                chunks=chunks,\n",
    "                metadata=doc_metadata,\n",
    "                stats=stats,\n",
    "                success=True\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            return PreprocessingResult(\n",
    "                document=None,\n",
    "                chunks=[],\n",
    "                metadata=None,\n",
    "                stats={},\n",
    "                success=False,\n",
    "                error=str(e)\n",
    "            )\n",
    "    \n",
    "    def process_directory(self, dir_path: str, recursive: bool = False) -> List[PreprocessingResult]:\n",
    "        \"\"\"디렉토리 내 모든 파일 전처리\"\"\"\n",
    "        results = []\n",
    "        path = Path(dir_path)\n",
    "        pattern = \"**/*\" if recursive else \"*\"\n",
    "        \n",
    "        supported_exts = self.file_parser.get_supported_extensions()\n",
    "        \n",
    "        for file_path in path.glob(pattern):\n",
    "            if file_path.is_file():\n",
    "                ext = file_path.suffix.lower().lstrip('.')\n",
    "                if ext in supported_exts:\n",
    "                    result = self.process_file(str(file_path))\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    if result.success:\n",
    "                        print(f\"✓ {file_path.name}: {result.stats['count']}개 청크\")\n",
    "                    else:\n",
    "                        print(f\"✗ {file_path.name}: {result.error}\")\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 전체 파이프라인 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이프라인 생성 완료!\n"
     ]
    }
   ],
   "source": [
    "# 파이프라인 생성\n",
    "pipeline = PreprocessingPipeline(\n",
    "    chunking_service=ChunkingService(\n",
    "        breakpoint_type=\"percentile\",\n",
    "        breakpoint_threshold=95\n",
    "    ),\n",
    "    normalizer=TextNormalizer(\n",
    "        remove_extra_whitespace=True,\n",
    "        remove_extra_newlines=True\n",
    "    ),\n",
    "    file_parser=UnifiedFileParser()\n",
    ")\n",
    "\n",
    "print(\"파이프라인 생성 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "전체 파이프라인 테스트\n",
      "==================================================\n",
      "✓ sample.pdf: 2개 청크\n",
      "✓ sample.xlsx: 1개 청크\n",
      "✓ sample.docx: 2개 청크\n",
      "✓ sample.json: 2개 청크\n",
      "✓ sample.txt: 2개 청크\n",
      "\n",
      "==================================================\n",
      "처리 결과 요약\n",
      "==================================================\n",
      "\n",
      "sample.pdf:\n",
      "  - 문서 ID: 8b3fc43a...\n",
      "  - 청크 수: 2\n",
      "  - 평균 크기: 978자\n",
      "  - 원본 → 정규화: 1,957 → 1,957자\n",
      "\n",
      "sample.xlsx:\n",
      "  - 문서 ID: 0a591f2d...\n",
      "  - 청크 수: 1\n",
      "  - 평균 크기: 1178자\n",
      "  - 원본 → 정규화: 1,178 → 1,178자\n",
      "\n",
      "sample.docx:\n",
      "  - 문서 ID: 5b0c6bc9...\n",
      "  - 청크 수: 2\n",
      "  - 평균 크기: 436자\n",
      "  - 원본 → 정규화: 878 → 878자\n",
      "\n",
      "sample.json:\n",
      "  - 문서 ID: 5879379b...\n",
      "  - 청크 수: 2\n",
      "  - 평균 크기: 864자\n",
      "  - 원본 → 정규화: 2,111 → 1,731자\n",
      "\n",
      "sample.txt:\n",
      "  - 문서 ID: 885b84f1...\n",
      "  - 청크 수: 2\n",
      "  - 평균 크기: 650자\n",
      "  - 원본 → 정규화: 1,307 → 1,306자\n",
      "\n",
      "총 청크 수: 9\n"
     ]
    }
   ],
   "source": [
    "# 전체 테스트 데이터 처리\n",
    "print(\"=\" * 50)\n",
    "print(\"전체 파이프라인 테스트\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = pipeline.process_directory(str(TEST_DATA_DIR))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"처리 결과 요약\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_chunks = 0\n",
    "for result in results:\n",
    "    if result.success:\n",
    "        total_chunks += result.stats['count']\n",
    "        print(f\"\\n{result.metadata.file_name}:\")\n",
    "        print(f\"  - 문서 ID: {result.metadata.doc_id[:8]}...\")\n",
    "        print(f\"  - 청크 수: {result.stats['count']}\")\n",
    "        print(f\"  - 평균 크기: {result.stats['avg_size']:.0f}자\")\n",
    "        print(f\"  - 원본 → 정규화: {result.stats['original_length']:,} → {result.stats['normalized_length']:,}자\")\n",
    "\n",
    "print(f\"\\n총 청크 수: {total_chunks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sample.pdf 청크 상세 ===\n",
      "\n",
      "--- Chunk 0 ---\n",
      "ID: 0f9b6f29...\n",
      "크기: 1093자\n",
      "내용 미리보기:\n",
      "RAG System Technical Report\n",
      "1. Executive Summary\n",
      "This document describes the RAG (Retrieval-Augmented Generation) system\n",
      "architecture and implementati...\n",
      "\n",
      "--- Chunk 1 ---\n",
      "ID: b187e39d...\n",
      "크기: 862자\n",
      "내용 미리보기:\n",
      "3. Performance Metrics\n",
      "The system has been evaluated on the following metrics:\n",
      "+-------------------+----------+--------+\n",
      "| Metric | Value | Target |\n",
      "+...\n"
     ]
    }
   ],
   "source": [
    "# 첫 번째 결과의 청크 상세 정보\n",
    "if results and results[0].success:\n",
    "    result = results[0]\n",
    "    print(f\"=== {result.metadata.file_name} 청크 상세 ===\")\n",
    "    \n",
    "    for i, chunk in enumerate(result.chunks[:3]):\n",
    "        print(f\"\\n--- Chunk {i} ---\")\n",
    "        print(f\"ID: {chunk.chunk_id[:8]}...\")\n",
    "        print(f\"크기: {chunk.char_count}자\")\n",
    "        print(f\"내용 미리보기:\")\n",
    "        preview = chunk.content[:150] + \"...\" if len(chunk.content) > 150 else chunk.content\n",
    "        print(preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## v2 모듈화 계획\n",
    "\n",
    "노트북 검증 완료 후, 다음 구조로 모듈화:\n",
    "\n",
    "```\n",
    "dev_v2/\n",
    "├── services/\n",
    "│   ├── chunking.py        # ChunkingService\n",
    "│   └── file_parser.py     # UnifiedFileParser, 개별 파서\n",
    "├── schemas/\n",
    "│   └── preprocessing.py   # RawDocument, Chunk, 메타데이터 모델\n",
    "├── preprocessing/\n",
    "│   ├── pipeline.py        # PreprocessingPipeline\n",
    "│   └── normalizer.py      # TextNormalizer\n",
    "└── config/\n",
    "    └── settings.py        # PreprocessingSettings 추가\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 검증 체크리스트\n",
    "\n",
    "- [ ] 각 파서가 정상 동작하는가?\n",
    "- [ ] 텍스트 정규화가 올바른가?\n",
    "- [ ] Semantic Chunking 품질이 적절한가? (평균 200~500자)\n",
    "- [ ] 메타데이터가 정확히 추출되는가?\n",
    "- [ ] End-to-End 파이프라인이 동작하는가?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
